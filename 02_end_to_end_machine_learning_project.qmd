---
title: Blog 2 
output:
  quarto::html_document:
    theme: cerulean
jupyter : python3
---

<!-- **The Art of Grouping - Clustering Chronicles.**
<br> 
*Written By Sandhya Vinukonda* -->

```{python}
import sys
print(sys.executable)
```


<!-- <table align="left">
  <td>
    <a href="https://colab.research.google.com/github/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
  </td>
  <td>
    <a target="_blank" href="https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb"><img src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a>
  </td>
</table>
<br>

# Introduction

In the vast landscape of machine learning, where chaos meets order, clustering emerges as the art of finding hidden patterns and groupings within data. Join me on a journey through the Clustering Chronicles, where algorithms play the role of data detectives, unraveling the mysteries of similarity and dissimilarity.

## The Intrigue of Clustering

Clustering algorithms are the protagonists in our narrative, seeking to discover the underlying structure within datasets. Their adventures lead us to unexpected insights and open new possibilities for understanding complex phenomena.

# K-means Clustering: Orchestrating the Drama

Our first character in the Clustering Chronicles is K-means clustering, a versatile algorithm that partitions data into distinct groups.

##### Code - 1 

In this visual representation, K-means has painted a picture of distinct clusters, each marked by a center represented by a red 'X'.

# Hierarchical Clustering: The Dendrogram's Tale

Our second protagonist is hierarchical clustering, an algorithm that builds a hierarchy of clusters, revealing the relationships between them.

##### Code - 2 

The dendrogram elegantly unfolds the hierarchical structure of our data, showcasing the relationships between different clusters.

## Evaluating the Drama: Metrics for Clustering Algorithms

As our clustering algorithms perform, we need metrics to evaluate their success. Let's explore some key metrics that add depth to the drama:

- **Silhouette Score:** Measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation).

- **Calinski-Harabasz Index:** Evaluates the ratio of the between-cluster variance to within-cluster variance.

#### Code 3 


# Choosing the Perfect Ensemble: Determining the Number of Clusters

In the Clustering Chronicles, the number of clusters is a critical plot point. The elbow method is a popular technique to find the optimal number.

#### Code 4 

The elbow method reveals the point where adding more clusters ceases to provide significant benefits, helping us find the optimal number.

# Conclusion

The Clustering Chronicles have introduced us to the dynamic world of grouping data, where algorithms like K-means and hierarchical clustering play the role of data detectives. As we navigate through the drama of clustering, we'll encounter more characters and unravel the complexity of unsupervised learning.

Stay tuned for the next chapter in our machine learning saga, where we dive into the twists and turns of linear and nonlinear regression.
 -->
