{"title":"Blog 4 : Classification","markdown":{"yaml":{"title":"Blog 4 : Classification ","author":"Sandhya Vinukonda","date":"2023-12-06","categories":["ML"]},"containsRefs":false,"markdown":"\n\nWhat does Classification do ? \n\nThe primary objective in supervised learning is to teach the model to predict the correct label or outcome for a given input. The model learns patterns and relationships in the training data, associating specific features with their corresponding labels.The training process involves exposing the model to the labeled training data, allowing it to adjust its internal parameters to make accurate predictions. Once trained, the model is then tested on new, unseen data (test data) to evaluate its generalization capabilities.The effectiveness of the supervised learning method is assessed by measuring its accuracy on the test data. Accuracy is calculated by comparing the model's predictions with the actual labels in the test set. This provides insights into how well the model performs on previously unseen examples.\n\nPicture 1 \n\nIn supervised learning, where we already know the correct answers, there are two main flavors: classification and regression. The key factor in deciding which one to use is the nature of the label data. If your label data consists of continuous values, then it's a regression task. Take predicting house prices as an example – here, the goal is to estimate a continuous value based on features like square footage, location, and the number of bedrooms.On the flip side, if your task involves predicting categorical outcomes, like whether a student will be accepted into a specific university or not, then it's a classification problem. In this scenario, the label data has distinct categories (admitted or not admitted), making it suitable for a classification approach.So, whether you're dealing with predicting prices or university admissions, understanding if your label data is continuous or categorical guides you in choosing between regression and classification for your supervised learning journey.\n\n\nEager Learners:\nEager learners proactively build a model from a training dataset before making predictions.\nThey invest more time during the training process to generalize better by learning the weights and relationships within the data.\nOnce the model is trained, making predictions is relatively quicker.\nExamples:\nLogistic Regression: A widely-used algorithm for binary and multiclass classification. It models the probability of a certain class.\nSupport Vector Machine (SVM): Efficiently classifies data points by finding the optimal hyperplane that separates different classes.\nDecision Trees: Tree-like models that make decisions based on features at each node, suitable for classification and regression.\nArtificial Neural Networks (ANN): Complex models inspired by the human brain, composed of interconnected nodes that process information in layers.\n\n\nLazy Learners (Instance-Based Learners):\nCharacteristics:\n\nLazy learners don't construct a model immediately; they memorize the training data.\nDuring prediction, they dynamically search for the nearest neighbor from the entire training dataset.\nThis approach can be slower during prediction but adapts well to changes in the dataset.\nExamples:\n\nK-Nearest Neighbor (KNN): Classifies data points based on the majority class of their k-nearest neighbors.\nCase-Based Reasoning: Makes predictions based on past cases, comparing the current problem to previously solved ones.\nUnderstanding these distinctions helps in choosing the right algorithm based on the characteristics of the dataset and the specific requirements of the problem at hand. Eager and lazy learners offer different trade-offs in terms of training time, prediction speed, and adaptability to changes in the dataset.\n\nPicture 2 \n\nTypes of Classifications : \n1. Binary : 0/1, spam and not spam, yes and no, negative and positive.\nExample : (long) \n2. Multi-class classification \nExample (one-vs-rest and one-vs-one)\n3. Multi-label classification \n4. Imbalanced classification \n\nLet’s get to coding! \n\n\nClassification takes place when the target variable is discrete. \nWhen a target variable isn't discrete, then regression takes place for the continuous varible. \n\nTypes of classification in ML : \n\nLet’s Start Coding \n\nPicture 3\n\nLet's look at a Machine Learning code example that shows binary classification of Blood Transfusion Service Centre. \n\nBefore we start coding let’s download the data and understand it. \nGo to link : https://archive.ics.uci.edu/dataset/176/blood+transfusion+service+center and click on download. \nAbout Data : This study focuses on the critical role of blood transfusions in saving lives, addressing the challenges of maintaining an adequate blood supply for medical needs. The research employs the RFMTC marketing model, a modified version of RFM, using the donor database of the Blood Transfusion Service Center in Hsin-Chu City, Taiwan. The study randomly selects 748 donors and gathers data on Recency, Frequency, Monetary contribution, Time, and a binary variable indicating blood donation in March 2007. This dataset forms the basis for \nbuilding an RFMTC model to enhance understanding and prediction in blood donation patterns.\n\nLet’s start coding : \n1. Import Libraries \n\ncode \n\n2. Read data and clean it \n\ncode \n\n3. Visualization of the data \n\ncode\n\n4. Splitting data into train and test\n\ncode\n\n5. Model : Logistic Regression\n\ncode\n\n6. Evaluate the model \n\ncode \n\n","srcMarkdownNoYaml":"\n\nWhat does Classification do ? \n\nThe primary objective in supervised learning is to teach the model to predict the correct label or outcome for a given input. The model learns patterns and relationships in the training data, associating specific features with their corresponding labels.The training process involves exposing the model to the labeled training data, allowing it to adjust its internal parameters to make accurate predictions. Once trained, the model is then tested on new, unseen data (test data) to evaluate its generalization capabilities.The effectiveness of the supervised learning method is assessed by measuring its accuracy on the test data. Accuracy is calculated by comparing the model's predictions with the actual labels in the test set. This provides insights into how well the model performs on previously unseen examples.\n\nPicture 1 \n\nIn supervised learning, where we already know the correct answers, there are two main flavors: classification and regression. The key factor in deciding which one to use is the nature of the label data. If your label data consists of continuous values, then it's a regression task. Take predicting house prices as an example – here, the goal is to estimate a continuous value based on features like square footage, location, and the number of bedrooms.On the flip side, if your task involves predicting categorical outcomes, like whether a student will be accepted into a specific university or not, then it's a classification problem. In this scenario, the label data has distinct categories (admitted or not admitted), making it suitable for a classification approach.So, whether you're dealing with predicting prices or university admissions, understanding if your label data is continuous or categorical guides you in choosing between regression and classification for your supervised learning journey.\n\n\nEager Learners:\nEager learners proactively build a model from a training dataset before making predictions.\nThey invest more time during the training process to generalize better by learning the weights and relationships within the data.\nOnce the model is trained, making predictions is relatively quicker.\nExamples:\nLogistic Regression: A widely-used algorithm for binary and multiclass classification. It models the probability of a certain class.\nSupport Vector Machine (SVM): Efficiently classifies data points by finding the optimal hyperplane that separates different classes.\nDecision Trees: Tree-like models that make decisions based on features at each node, suitable for classification and regression.\nArtificial Neural Networks (ANN): Complex models inspired by the human brain, composed of interconnected nodes that process information in layers.\n\n\nLazy Learners (Instance-Based Learners):\nCharacteristics:\n\nLazy learners don't construct a model immediately; they memorize the training data.\nDuring prediction, they dynamically search for the nearest neighbor from the entire training dataset.\nThis approach can be slower during prediction but adapts well to changes in the dataset.\nExamples:\n\nK-Nearest Neighbor (KNN): Classifies data points based on the majority class of their k-nearest neighbors.\nCase-Based Reasoning: Makes predictions based on past cases, comparing the current problem to previously solved ones.\nUnderstanding these distinctions helps in choosing the right algorithm based on the characteristics of the dataset and the specific requirements of the problem at hand. Eager and lazy learners offer different trade-offs in terms of training time, prediction speed, and adaptability to changes in the dataset.\n\nPicture 2 \n\nTypes of Classifications : \n1. Binary : 0/1, spam and not spam, yes and no, negative and positive.\nExample : (long) \n2. Multi-class classification \nExample (one-vs-rest and one-vs-one)\n3. Multi-label classification \n4. Imbalanced classification \n\nLet’s get to coding! \n\n\nClassification takes place when the target variable is discrete. \nWhen a target variable isn't discrete, then regression takes place for the continuous varible. \n\nTypes of classification in ML : \n\nLet’s Start Coding \n\nPicture 3\n\nLet's look at a Machine Learning code example that shows binary classification of Blood Transfusion Service Centre. \n\nBefore we start coding let’s download the data and understand it. \nGo to link : https://archive.ics.uci.edu/dataset/176/blood+transfusion+service+center and click on download. \nAbout Data : This study focuses on the critical role of blood transfusions in saving lives, addressing the challenges of maintaining an adequate blood supply for medical needs. The research employs the RFMTC marketing model, a modified version of RFM, using the donor database of the Blood Transfusion Service Center in Hsin-Chu City, Taiwan. The study randomly selects 748 donors and gathers data on Recency, Frequency, Monetary contribution, Time, and a binary variable indicating blood donation in March 2007. This dataset forms the basis for \nbuilding an RFMTC model to enhance understanding and prediction in blood donation patterns.\n\nLet’s start coding : \n1. Import Libraries \n\ncode \n\n2. Read data and clean it \n\ncode \n\n3. Visualization of the data \n\ncode\n\n4. Splitting data into train and test\n\ncode\n\n5. Model : Logistic Regression\n\ncode\n\n6. Evaluate the model \n\ncode \n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title-block-banner":true,"title":"Blog 4 : Classification ","author":"Sandhya Vinukonda","date":"2023-12-06","categories":["ML"]},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}