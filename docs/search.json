[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog101",
    "section": "",
    "text": "Blog 3 : Clustering\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\n  \n\n\n\n\nBlog 4 : Classification\n\n\n\n\n\n\n\nML\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nSandhya Vinukonda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Blog 4 : Classification",
    "section": "",
    "text": "What does Classification do ?\nThe primary objective in supervised learning is to teach the model to predict the correct label or outcome for a given input. The model learns patterns and relationships in the training data, associating specific features with their corresponding labels.The training process involves exposing the model to the labeled training data, allowing it to adjust its internal parameters to make accurate predictions. Once trained, the model is then tested on new, unseen data (test data) to evaluate its generalization capabilities.The effectiveness of the supervised learning method is assessed by measuring its accuracy on the test data. Accuracy is calculated by comparing the model’s predictions with the actual labels in the test set. This provides insights into how well the model performs on previously unseen examples.\nPicture 1\nIn supervised learning, where we already know the correct answers, there are two main flavors: classification and regression. The key factor in deciding which one to use is the nature of the label data. If your label data consists of continuous values, then it’s a regression task. Take predicting house prices as an example – here, the goal is to estimate a continuous value based on features like square footage, location, and the number of bedrooms.On the flip side, if your task involves predicting categorical outcomes, like whether a student will be accepted into a specific university or not, then it’s a classification problem. In this scenario, the label data has distinct categories (admitted or not admitted), making it suitable for a classification approach.So, whether you’re dealing with predicting prices or university admissions, understanding if your label data is continuous or categorical guides you in choosing between regression and classification for your supervised learning journey.\nEager Learners: Eager learners proactively build a model from a training dataset before making predictions. They invest more time during the training process to generalize better by learning the weights and relationships within the data. Once the model is trained, making predictions is relatively quicker. Examples: Logistic Regression: A widely-used algorithm for binary and multiclass classification. It models the probability of a certain class. Support Vector Machine (SVM): Efficiently classifies data points by finding the optimal hyperplane that separates different classes. Decision Trees: Tree-like models that make decisions based on features at each node, suitable for classification and regression. Artificial Neural Networks (ANN): Complex models inspired by the human brain, composed of interconnected nodes that process information in layers.\nLazy Learners (Instance-Based Learners): Characteristics:\nLazy learners don’t construct a model immediately; they memorize the training data. During prediction, they dynamically search for the nearest neighbor from the entire training dataset. This approach can be slower during prediction but adapts well to changes in the dataset. Examples:\nK-Nearest Neighbor (KNN): Classifies data points based on the majority class of their k-nearest neighbors. Case-Based Reasoning: Makes predictions based on past cases, comparing the current problem to previously solved ones. Understanding these distinctions helps in choosing the right algorithm based on the characteristics of the dataset and the specific requirements of the problem at hand. Eager and lazy learners offer different trade-offs in terms of training time, prediction speed, and adaptability to changes in the dataset.\nPicture 2\nTypes of Classifications : 1. Binary : 0/1, spam and not spam, yes and no, negative and positive. Example : (long) 2. Multi-class classification Example (one-vs-rest and one-vs-one) 3. Multi-label classification 4. Imbalanced classification\nLet’s get to coding!\nClassification takes place when the target variable is discrete. When a target variable isn’t discrete, then regression takes place for the continuous varible.\nTypes of classification in ML :\nLet’s Start Coding\nPicture 3\nLet’s look at a Machine Learning code example that shows binary classification of Blood Transfusion Service Centre.\nBefore we start coding let’s download the data and understand it. Go to link : https://archive.ics.uci.edu/dataset/176/blood+transfusion+service+center and click on download. About Data : This study focuses on the critical role of blood transfusions in saving lives, addressing the challenges of maintaining an adequate blood supply for medical needs. The research employs the RFMTC marketing model, a modified version of RFM, using the donor database of the Blood Transfusion Service Center in Hsin-Chu City, Taiwan. The study randomly selects 748 donors and gathers data on Recency, Frequency, Monetary contribution, Time, and a binary variable indicating blood donation in March 2007. This dataset forms the basis for building an RFMTC model to enhance understanding and prediction in blood donation patterns.\nLet’s start coding : 1. Import Libraries\ncode\n\nRead data and clean it\n\ncode\n\nVisualization of the data\n\ncode\n\nSplitting data into train and test\n\ncode\n\nModel : Logistic Regression\n\ncode\n\nEvaluate the model\n\ncode"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Blog 3 : Clustering",
    "section": "",
    "text": "Let’s talk about a fascinating part of machine learning and unsupervised learning. In this world, there’s a standout technique known as clustering, where we work with data that doesn’t come pre-labeled. Instead, we group similar data points into clusters based on common traits or patterns. This blog will take you through the basics of clustering, its types, and zoom in on two popular clustering buddies: K Means and Hierarchical Clustering.\nUnderstanding Clustering:\nClustering is like a detective trying to find hidden connections in a bunch of clues (data points). The cool thing is, no one is telling the detective what to look for – the algorithm figures it out on its own.\nTypes of Clustering:\nHard Clustering:\nImagine putting each data point into one exclusive group. That’s hard clustering for you. Classic example: K Means.\nSoft Clustering:\nNow, think of data points being chill and belonging to multiple groups with varying levels of closeness. That’s soft clustering. Fuzzy C-Means is a soft clustering star.\nDifferent Clustering Models:\nPicture 1\nConnectivity Models: These algorithms look at how close data points are to each other. Example: Hierarchical Clustering.\nCentroid Models: These focus on the center points of clusters. K Means is a big player here.\nDistribution Models: Assume that data points follow a common pattern. Think of Gaussian Mixture Models (GMM).\nDensity Models: These spot clusters based on where data points crowd up. A famous one is DBSCAN.\nProminent Clustering Algorithms: K Means Clustering: It’s like dividing data into clubs based on the average characteristics of each group. Simple and efficient – a go-to for many.\nHierarchical Clustering: This one builds a family tree of clusters, like tracing your ancestry. It’s cool because it allows for nested or overlapping groups.\nDifference Between K Means and Hierarchical Clustering: Here’s the lowdown on how K Means and Hierarchical Clustering differ:\nNature of Clusters: K Means makes clear-cut groups with no sharing allowed. Hierarchical Clustering creates a family tree where groups can overlap or nest.\nNumber of Clusters: K Means needs to know how many groups you want before it starts. Hierarchical Clustering is more flexible – it generates a tree, and you can decide how many groups later.\nComputation Complexity: K Means is quicker with the calculations, good for big datasets. Hierarchical Clustering can take more time, especially with lots of data. Conclusion: In the unsupervised learning world, clustering is like a superhero revealing hidden patterns in data without anyone giving it a roadmap. Knowing the ins and outs of clustering, the different types, and the quirks of K Means and Hierarchical Clustering gives you a powerful toolkit for finding the stories hidden in your data. As technology races forward, the possibilities for unsupervised learning and clustering are endless, promising discoveries in fields from healthcare to finance.\nLet’s Start Coding\n\nImport Libraries\nRead data and clean it\nModel : K Means"
  }
]